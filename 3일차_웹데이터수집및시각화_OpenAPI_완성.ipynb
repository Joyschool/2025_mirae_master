{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Joyschool/2025_mirae_master/blob/main/3%EC%9D%BC%EC%B0%A8_%EC%9B%B9%EB%8D%B0%EC%9D%B4%ED%84%B0%EC%88%98%EC%A7%91%EB%B0%8F%EC%8B%9C%EA%B0%81%ED%99%94_OpenAPI_%EC%99%84%EC%84%B1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "46265587",
      "metadata": {
        "id": "46265587"
      },
      "source": [
        "# 웹 데이터 수집 및 시각화"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "go-R3kQSaSrk",
      "metadata": {
        "id": "go-R3kQSaSrk"
      },
      "source": [
        "## 2. OpenAPI를 이용한 데이터 수집"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0b715273",
      "metadata": {
        "id": "0b715273"
      },
      "source": [
        "- <a href=\"#1)OpenAP를이용한데이터수집\">1) OpenAPI를 이용한 데이터 수집</a>\n",
        "- <a href=\"#2)뉴스데이터워드클라우드만들기\">2) 뉴스데이터 워드클라우드 만들기</a>\n",
        "- <a href=\"#3)웹이미지수집하기\">3) 웹 이미지 수집하기</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "VGDzYiKZhR0U",
      "metadata": {
        "id": "VGDzYiKZhR0U"
      },
      "source": [
        "### #그래프에서 한글사용하는 방법\n",
        "- **(코랩에서)한글폰트 설치하기**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hwAJP1nahhrF",
      "metadata": {
        "id": "hwAJP1nahhrF"
      },
      "outputs": [],
      "source": [
        "!sudo apt-get install -y fonts-nanum\n",
        "!sudo fc-cache -fv\n",
        "!rm ~/.cache/matplotlib -rf\n",
        "\n",
        "# 코랩에서 위 코드를 실행시킨 후  반드시 코랩 메뉴: \"런타임>세션 다시 시작\" 합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4YrZX8Hf1hQ8",
      "metadata": {
        "id": "4YrZX8Hf1hQ8"
      },
      "source": [
        "- **한글 폰트 지정하기**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "CfuS0iwGhhee",
      "metadata": {
        "id": "CfuS0iwGhhee"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ld3tL_Sh1gXu",
      "metadata": {
        "id": "ld3tL_Sh1gXu"
      },
      "outputs": [],
      "source": [
        "# 코랩에서 한글 폰트 종류와 이름이 win과 다를 수 있다!!!\n",
        "# 코랩: NanumGothic, 윈도우: Malgun Gothic\n",
        "import matplotlib.pyplot as plt\n",
        "plt.rcParams.update({'font.family': 'NanumGothic',\n",
        "                     'font.size': 12,\n",
        "                     'figure.figsize': (6, 4),\n",
        "                     'axes.unicode_minus':  False }) # 폰트 설정"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6d0c04ae",
      "metadata": {
        "id": "6d0c04ae"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f4f12800",
      "metadata": {
        "id": "f4f12800"
      },
      "source": [
        "### <a name=\"1)OpenAP를이용한데이터수집\">1) OpenAPI를 이용한 데이터 수집</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db60e9f8",
      "metadata": {
        "id": "db60e9f8"
      },
      "source": [
        "#### 1.네이버 OpenAPI 신청하기\n",
        "* 네이버 검색(책, 뉴스, 쇼핑)\n",
        "    - 네이버 OpenAPI 소개: https://developers.naver.com/products/intro/plan/\n",
        "    - 개발 가이드 보기: https://developers.naver.com/docs/serviceapi/search/news\n",
        "    - OpenAPI 신청하기: https://developers.naver.com/apps/#/register"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6093ed93",
      "metadata": {
        "id": "6093ed93"
      },
      "source": [
        "#### 2.네이버 OpenAPI 사용하기\n",
        "* 검색\n",
        "    - 1.책 검색\n",
        "    - 2.뉴스 검색\n",
        "    - 3.쇼핑 검색"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0245934c",
      "metadata": {
        "id": "0245934c"
      },
      "source": [
        "#### [실습] : 네이버 검색 API 사용하여 데이터 수집하기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "03fe1289",
      "metadata": {
        "id": "03fe1289"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import urllib.request\n",
        "import datetime\n",
        "import time\n",
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "now = datetime.datetime.now()\n",
        "datas = []   #csv파일을 위한 변수\n",
        "\n",
        "client_id = ''  # 자신의 client_id\n",
        "client_pw = ''  # 자신의 client_secret\n",
        "\n",
        "\n",
        "#[CODE 1]\n",
        "def get_RequestUrl(url):\n",
        "    req = urllib.request.Request(url)\n",
        "    req.add_header(\"X-Naver-Client-Id\", client_id)\n",
        "    req.add_header(\"X-Naver-Client-Secret\", client_pw)\n",
        "\n",
        "    try:\n",
        "        response = urllib.request.urlopen(req)\n",
        "        if response.getcode() == 200:\n",
        "            print(f\"[{now.strftime('%Y년%m월%d일 %H시%M분%S초')}] Url Request Success\")\n",
        "            return response.read().decode('utf-8')\n",
        "\n",
        "    except Exception as e:\n",
        "#         if response.getcode() == 400 and datas:\n",
        "#             return None\n",
        "        print(e)\n",
        "        print(f\"[{now.strftime('%Y년%m월%d일 %H시%M분%S초')}] Error for URL : {url}\" )\n",
        "        return None\n",
        "\n",
        "\n",
        "#[CODE 2]  네이버 검색 API\n",
        "def get_NaverSearch(node, keyword, start, display):\n",
        "    base = \"https://openapi.naver.com/v1/search\"\n",
        "    node = \"/%s.json\" % node\n",
        "    parameters = \"?query=%s&start=%s&display=%s\" % (urllib.parse.quote(keyword), start, display)\n",
        "\n",
        "    url = base + node + parameters\n",
        "    responseDecode = get_RequestUrl(url)   #[CODE 1]\n",
        "\n",
        "    if (responseDecode == None):\n",
        "        return None\n",
        "    else:\n",
        "        return json.loads(responseDecode)\n",
        "\n",
        "#[CODE 3]\n",
        "def get_PostData(node, post, jsonResult, cnt):\n",
        "    if node == 'book':\n",
        "        data = {'제목':post['title'],\n",
        "                '저자':post['author'],\n",
        "                '출판사':post['publisher'],\n",
        "                '출간일':post['pubdate'],\n",
        "                '링크':post['link'],\n",
        "                '이미지':post['image']}\n",
        "    elif node == 'news':\n",
        "        data = {'제목':post['title'],\n",
        "                '링크':post['originallink'],\n",
        "                '내용':post['description']}\n",
        "    elif node == 'shop':\n",
        "        data = {'제목':post['title'],\n",
        "                '브랜드':post['brand'],\n",
        "                '제조사':post['maker'],\n",
        "                '가격':post['lprice'],\n",
        "                '이미지':post['image']}\n",
        "\n",
        "    jsonResult.append(data)\n",
        "    datas.append(data)\n",
        "\n",
        "    return\n",
        "\n",
        "#[CODE 0]\n",
        "def main():\n",
        "    global node, keyword\n",
        "\n",
        "    nodeType = '''수집할 대상 입니다.\\n 1.book,  2.news,  3.shop'''\n",
        "    print('-'*30)\n",
        "    print(nodeType)\n",
        "    print('-'*30)\n",
        "    node = input('번호를 선택하세요.[1:책, 2:뉴스, 3:쇼핑]')\n",
        "    if node == '1': node = 'book'\n",
        "    elif node == '2': node = 'news'\n",
        "    elif node == '3': node = 'shop'\n",
        "    else:\n",
        "        node == '1'\n",
        "        node = 'book'\n",
        "\n",
        "    keyword = input(f'{node} 검색할 검색어를 입력하세요: ')\n",
        "    print('-'*30)\n",
        "\n",
        "    display, cnt = 100, 0\n",
        "    jsonResult = []\n",
        "    jsonResponse = get_NaverSearch(node, keyword, 1, display)  #[CODE 2]\n",
        "    total = jsonResponse['total']\n",
        "\n",
        "    while ((jsonResponse != None) and (jsonResponse['display'] != 0)):\n",
        "        for post in jsonResponse['items']:\n",
        "            cnt += 1\n",
        "            get_PostData(node, post, jsonResult, cnt)  #[CODE 3]\n",
        "\n",
        "        start = jsonResponse['start'] + jsonResponse['display']\n",
        "        jsonResponse = get_NaverSearch(node, keyword, start, total)\n",
        "#     print(f'가져올 데이터 : {total} 건')\n",
        "\n",
        "    # with open(f'naver_{node}_{keyword}', 'w', encoding='utf8') as outfile:\n",
        "    #     jsonFile = json.dumps(jsonResult,  indent=4, sort_keys=True,  ensure_ascii=False)\n",
        "\n",
        "    #     outfile.write(jsonFile)\n",
        "\n",
        "    print(\"가져온 데이터 : %d 건\" %(cnt))\n",
        "\n",
        "\n",
        "\n",
        "    # csv 파일로 저장하기\n",
        "    file = f'naver_{node}_{keyword}.csv'\n",
        "    df = pd.DataFrame(datas)\n",
        "    df.to_csv(file, index=False, encoding=\"utf-8-sig\")\n",
        "    print(f'naver_{node}_{keyword}.csv SAVED')\n",
        "    return df\n",
        "\n",
        "#-----------\n",
        "# 시작\n",
        "#-----------\n",
        "df = main()\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6lHcEMfHjLfk",
      "metadata": {
        "id": "6lHcEMfHjLfk"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7a27fa6d",
      "metadata": {
        "id": "7a27fa6d"
      },
      "source": [
        "### [실습] 수집된 데이터 분석하기(책)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Q1.수집된 [책] 데이터에서 저자가 '한강'인 도서만 출력하세요."
      ],
      "metadata": {
        "id": "7-iBiYVgG0nK"
      },
      "id": "7-iBiYVgG0nK"
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Q1. 컬럼 목록"
      ],
      "metadata": {
        "id": "9NmfuxT7NFgs"
      },
      "id": "9NmfuxT7NFgs"
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "id": "rFR7QZGnNIWr"
      },
      "id": "rFR7QZGnNIWr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Q2. 저자 컬럼의 고유한 값과 빈도수"
      ],
      "metadata": {
        "id": "1srjh52dNFj9"
      },
      "id": "1srjh52dNFj9"
    },
    {
      "cell_type": "code",
      "source": [
        "df['저자'].value_counts()"
      ],
      "metadata": {
        "id": "fH9PL5KSNIu6"
      },
      "id": "fH9PL5KSNIu6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Q3. 저자가 '한강'인 도서 출력"
      ],
      "metadata": {
        "id": "NJbvM2DRNHti"
      },
      "id": "NJbvM2DRNHti"
    },
    {
      "cell_type": "code",
      "source": [
        "df_한강 = df[df['저자']=='한강']\n",
        "df_한강"
      ],
      "metadata": {
        "id": "f_olwjysNJE6"
      },
      "id": "f_olwjysNJE6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Q4.'한강' 저자의 최근 작품 정보(출간일, 책제목, 출판사 등)"
      ],
      "metadata": {
        "id": "ixfySrGcNH63"
      },
      "id": "ixfySrGcNH63"
    },
    {
      "cell_type": "code",
      "source": [
        "df_한강.sort_values(by='출간일', ascending=False).loc[0]   # 내림차순으로 정렬하고 첫번째 인덱스에 해당하는 데이터"
      ],
      "metadata": {
        "id": "KGS44dCIRWU9"
      },
      "id": "KGS44dCIRWU9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_idx = df_한강['출간일'].idxmax()   # 해당 컬럼의 max 값의 행 인덱스\n",
        "df_한강.loc[max_idx]   # 주어진 인덱스에 해당하는 레코드 정보 출력"
      ],
      "metadata": {
        "id": "-hyySTU1NKFK"
      },
      "id": "-hyySTU1NKFK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Q5.'한강' 저자의 최초 작품 정보(출간일, 책제목, 출판사 등)"
      ],
      "metadata": {
        "id": "DeiZH-17NHwW"
      },
      "id": "DeiZH-17NHwW"
    },
    {
      "cell_type": "code",
      "source": [
        "df_한강 = df_한강[df_한강['출간일'] != '']  # 출간일 정보가 없는 것 제거\n",
        "min_idx = df_한강['출간일'].idxmin()   # 해당 컬럼의 min 값의 행 인덱스\n",
        "df_한강.loc[min_idx]   # 주어진 인덱스에 해당하는 레코드 정보 출력"
      ],
      "metadata": {
        "id": "oNN8JqR0OxAM"
      },
      "id": "oNN8JqR0OxAM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "aa56f180",
      "metadata": {
        "id": "aa56f180"
      },
      "source": [
        "-------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "OARyiX8OjSDk",
      "metadata": {
        "id": "OARyiX8OjSDk"
      },
      "source": [
        "### <a name=\"2))뉴스데이터워드클라우드만들기\">2) 뉴스데이터 워드클라우드 만들기</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "iSHlDdMOkhPa",
      "metadata": {
        "id": "iSHlDdMOkhPa"
      },
      "source": [
        "#### 1.라이브러리 설치"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "yW92MW8gjwcV",
      "metadata": {
        "id": "yW92MW8gjwcV"
      },
      "outputs": [],
      "source": [
        "# 자연어처리 형태소 분석 및 토큰화\n",
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "xG_Jjc3Vjyvn",
      "metadata": {
        "id": "xG_Jjc3Vjyvn"
      },
      "outputs": [],
      "source": [
        "# (한글)자연어처리 형태소 분석 및 토큰화\n",
        "# (WinOS에서는 konlpy를 사용하기 위해 JDK를 설치해야 오류가 없어 코랩에서 실행함)\n",
        "!pip install konlpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "QnXMfe_tj2Tr",
      "metadata": {
        "id": "QnXMfe_tj2Tr"
      },
      "outputs": [],
      "source": [
        "# html tag parsing을 위한 라이브러리\n",
        "!pip install beautifulsoup4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "MYcUubDGfUjr",
      "metadata": {
        "id": "MYcUubDGfUjr"
      },
      "outputs": [],
      "source": [
        "# 사이킷런 CountVectorizer클래스 사용 : 단어 빈도수 추출\n",
        "!pip install scikit-learn"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "tQ6-YXHGkkCN",
      "metadata": {
        "id": "tQ6-YXHGkkCN"
      },
      "source": [
        "#### 2.한글 테스트 자연어 처리(Text Cleansing)\n",
        "- 앞에서 OpenAPI를 사용하여 뉴스 데이터를 가져온 뒤에 아래 코드를 실행시키도록 한다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bBQc3PLwjLzQ",
      "metadata": {
        "id": "bBQc3PLwjLzQ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "import konlpy\n",
        "import nltk\n",
        "\n",
        "# 추가\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "def clean_korean_documents(documents):\n",
        "\n",
        "    # 결측값을 제거\n",
        "    for i, document in enumerate(documents):\n",
        "        if pd.isnull(document):\n",
        "            documents[i] = \"\"\n",
        "\n",
        "\n",
        "    #텍스트 정제 (HTML 태그 제거)\n",
        "    for i, document in enumerate(documents):\n",
        "        document = BeautifulSoup(document, 'html.parser').text\n",
        "        documents[i] = document\n",
        "\n",
        "    #텍스트 정제 (특수기호 제거)\n",
        "    for i, document in enumerate(documents):\n",
        "        document = re.sub(r'[^ ㄱ-ㅣ가-힣]', '', document) #특수기호 제거\n",
        "        documents[i] = document\n",
        "\n",
        "    #텍스트 정제 (형태소 분석)\n",
        "    for i, document in enumerate(documents):\n",
        "        okt = konlpy.tag.Okt()\n",
        "        clean_words = []\n",
        "        for word in okt.pos(document, stem=True):\n",
        "            if word[1] in ['Noun', 'Verb', 'Adjective']:\n",
        "                clean_words.append(word[0])\n",
        "        documents[i] = ' '.join(clean_words)\n",
        "\n",
        "    #텍스트 정제 (불용어 제거)\n",
        "    df = pd.read_csv(\n",
        "        'https://raw.githubusercontent.com/cranberryai/todak_todak_python/master/machine_learning_text/clean_korean_documents/korean_stopwords.txt',\n",
        "        header=None\n",
        "    )\n",
        "    stopwords = df[0].str.strip().tolist()\n",
        "\n",
        "    for i, document in enumerate(documents):\n",
        "        clean_words = []\n",
        "        for word in nltk.tokenize.word_tokenize(document):\n",
        "            if word not in stopwords:\n",
        "                clean_words.append(word)\n",
        "        documents[i] = ' '.join(clean_words)\n",
        "\n",
        "    return documents\n",
        "\n",
        "\n",
        "def clean_target_feature(data):\n",
        "    x_data = data.to_list()\n",
        "    return clean_korean_documents(x_data)\n",
        "\n",
        "\n",
        "\n",
        "# ---------------------\n",
        "# 1.클린징할 파일 가져오기\n",
        "# 앞에서 저장한 뉴스 데이터 파일 이름\n",
        "file = 'naver_news_노벨상.csv'  # 앞에서 만들어진 파일 사용하기\n",
        "df = pd.read_csv(file)\n",
        "\n",
        "# 2.클린징할 컬럼 선택해서 클린징하기\n",
        "# (앞 OpenAPI검색에서 2.news 검색을 한 후 실행하기)\n",
        "df['제목'] = clean_target_feature(df['제목'])  # 기사 제목\n",
        "df['내용'] = clean_target_feature(df['내용'])  # 기사 내용\n",
        "\n",
        "# 3.클린징 결과 csv 파일로 저장하기\n",
        "file = 'naver_news_노벨상_clean.csv'\n",
        "df.to_csv(file, index=False, encoding=\"utf-8-sig\")\n",
        "print(f'{file} SAVED')\n",
        "\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4k38KlXmtWPn",
      "metadata": {
        "id": "4k38KlXmtWPn"
      },
      "source": [
        "#### 3.워드 클라우드 만들기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vkCWhYlgt8c2",
      "metadata": {
        "id": "vkCWhYlgt8c2"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "plt.rcParams['figure.figsize'] = (10,6)\n",
        "\n",
        "# 말뭉치를 토큰화하여 빈도수 가져오기\n",
        "def get_wordTokenCount(corpus):\n",
        "    from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "    # 말뭉치를 토큰화하기\n",
        "    vect = CountVectorizer().fit(corpus)\n",
        "    count = vect.transform(corpus).toarray().sum(axis=0)\n",
        "\n",
        "    # 토큰 빈도수로 정렬하고 토큰명 추출\n",
        "    idx = np.argsort(-count)  # 내림 정렬하여 인덱스 반환: 토큰의 인덱스\n",
        "    count = count[idx]        # 토큰의 빈도수\n",
        "    feature_name = np.array(vect.get_feature_names_out())[idx]  # 토큰값\n",
        "\n",
        "    # 빈도수 많은 순서대로 토큰명 10개만 출력\n",
        "    print(list(zip(feature_name, count))[:10])\n",
        "\n",
        "    return feature_name, count\n",
        "\n",
        "\n",
        "# 단어(토큰) 빈도수 막대 그래프 그리기\n",
        "def draw_wordTokenCountGraph(data, freq):\n",
        "    plt.bar(data, freq)\n",
        "    plt.grid()\n",
        "    plt.show()\n",
        "\n",
        "    # 그래프 그림 저장히기\n",
        "    plt.savefig(f'./token_bar_graph.png')\n",
        "\n",
        "# 워드클라우드 만들기\n",
        "def make_wordcloud(feature_name, count):\n",
        "    # 한글 폰트 경로를 설정\n",
        "    # font_path = 'NanumGothic'  #/usr/share/fonts/truetype/nanum/NanumGothic.ttf  #코랩\n",
        "    font_path = '/usr/share/fonts/truetype/nanum/NanumGothic.ttf'  #/usr/share/fonts/truetype/nanum/NanumGothic.ttf  #코랩\n",
        "\n",
        "    # (토큰명, 빈도수) 딕셔너리 타입으로 변환\n",
        "    data = dict(zip(feature_name, count))\n",
        "\n",
        "    # 워드클라우드로 그래프로 시각화\n",
        "    wc = WordCloud(width = 1000, height = 600, background_color=\"black\", font_path=font_path)\n",
        "    plt.imshow(wc.generate_from_frequencies(data)) #딕셔너리\n",
        "    plt.axis(\"off\")\n",
        "    plt.show()\n",
        "\n",
        "    # 이미지 파일로 저장하기\n",
        "    wc.to_file(f'워드클라우드.png')\n",
        "\n",
        "\n",
        "# 0.앞에서 클린징한 파일 사용하기\n",
        "df = pd.read_csv('naver_news_노벨상_clean.csv')\n",
        "\n",
        "# 1.텍스트 말뭉치(corpus) 데이터 지정하기\n",
        "corpus = df['제목'].to_list()\n",
        "# print(corpus)\n",
        "\n",
        "# 2.말뭉치를 토큰화하여 빈도수 가져오기\n",
        "feature_name, count = get_wordTokenCount(corpus)\n",
        "\n",
        "# 3.단어(토큰) 빈도수 막대 그래프 그리기(상위 10개)\n",
        "# draw_wordTokenCountGraph(feature_name[:10], count[:10])\n",
        "\n",
        "# 3.워드 클라우드 만들기\n",
        "make_wordcloud(feature_name, count)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Ajcnidsqs2hR",
      "metadata": {
        "id": "Ajcnidsqs2hR"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "59fa45de",
      "metadata": {
        "id": "59fa45de"
      },
      "source": [
        "### <a name=\"3)웹이미지수집하기\">3) 웹 이미지 수집하기</a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('naver_shop_여성니트.csv')\n",
        "df.head(5)"
      ],
      "metadata": {
        "id": "tdyLFzpXY7B7"
      },
      "id": "tdyLFzpXY7B7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "44d3fd31",
      "metadata": {
        "id": "44d3fd31"
      },
      "source": [
        "#### 1.웹 이미지 화면에 출력하기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "337a091d",
      "metadata": {
        "id": "337a091d",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "from IPython.display import Image\n",
        "Image(url='https://shopping-phinf.pstatic.net/main_8201676/82016764964.17.jpg')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0680e362",
      "metadata": {
        "id": "0680e362"
      },
      "source": [
        "#### 2.웹 이미지 파일로 저장하기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a580299b",
      "metadata": {
        "id": "a580299b"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from io import BytesIO\n",
        "from PIL import Image\n",
        "\n",
        "# 다운받을 이미지 url\n",
        "urls = [\n",
        "    \"https://shopping-phinf.pstatic.net/main_8201676/82016764964.17.jpg\",\n",
        "    \"https://shopping-phinf.pstatic.net/main_8339525/83395253385.13.jpg\"\n",
        "]\n",
        "# 파일로 저장하기\n",
        "for idx, url in enumerate(urls):\n",
        "    res = requests.get(url)                   # url 요청\n",
        "    img = Image.open(BytesIO(res.content))    # 응답결과(res.content)바이트파일 이미지 파일로 오픈\n",
        "    img.save(f'./testimg_{idx}.png', 'png')   # 'png'이미지로(만) 저장"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cfdf3dbf",
      "metadata": {
        "id": "cfdf3dbf"
      },
      "source": [
        "#### [실습] : 웹 이미지 수집하기\n",
        "- 앞에서 저장한 쇼핑 목록에 있는 URL을 이용하여 이미지 저장하기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9cc4ce14",
      "metadata": {
        "id": "9cc4ce14"
      },
      "outputs": [],
      "source": [
        "# 이미지 다운로드 하기\n",
        "import pandas as pd\n",
        "import requests\n",
        "from io import BytesIO\n",
        "from PIL import Image\n",
        "import time\n",
        "import os\n",
        "\n",
        "\n",
        "def getImageUrl(file):\n",
        "    print(f'읽은 파일명: {file}')\n",
        "    df = pd.read_csv(file, encoding='utf-8') # 이미지가 있는 쇼핑 파일 불러오기\n",
        "    return df['이미지'].to_list()\n",
        "\n",
        "def createDirectory(directory): # 다운받을 이미지 폴더 만들기\n",
        "    try:\n",
        "        if not os.path.exists(directory):\n",
        "            print(f'{directory} 폴더가 생성되었습니다.')\n",
        "            os.makedirs(directory)\n",
        "        print(f'이미지 폴더 위치 : {directory}')\n",
        "    except OSError:\n",
        "        print(\"Error: Failed to create the directory.\")\n",
        "\n",
        "def downloadImageFile(urls, imgfolder):\n",
        "    start = time.time()             # 이미지 다운로드 속도 time check\n",
        "    for idx, url in enumerate(urls):\n",
        "        if idx == MAX:\n",
        "            break  # MAX 건수만 처리하기\n",
        "        res = requests.get(url)     # request.get 요청\n",
        "#         print(f'[{idx+1:2>}][{time.time() - start}] : {url}')  # 이미지 다운로드 시간 체크\n",
        "        print(f'[{idx+1:0>2}] : {url}')  # 이미지 다운로드 시간 체크\n",
        "        img = Image.open(BytesIO(res.content))  #Img open\n",
        "        img.save(f'{imgfolder}testimage_{idx}.png', 'png')\n",
        "    return idx\n",
        "\n",
        "\n",
        "# 자신에게 맞게 폴더/파일 위치 정보를 수정한다.\n",
        "shop_image = 'naver_shop_여성니트.csv'  # 앞에서 자신이 검색한 쇼핑 파일\n",
        "image_folder = './image_download/'      # 다운받을 이미지 폴더\n",
        "MAX = 10                              # 이미지 파일 다운로드 건수\n",
        "\n",
        "# 이미지 다운로드 하기\n",
        "urls = getImageUrl(shop_image)                # 이미지 URL 목록 가져오기\n",
        "createDirectory(image_folder)              # 다운받을 이미지 폴더 만들기\n",
        "totalcnt = downloadImageFile(urls, image_folder)      # 이미지 다운로드하기\n",
        "print(f'총 다운로드 건수: {totalcnt}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fb18b237",
      "metadata": {
        "id": "fb18b237"
      },
      "source": [
        "---------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dd06d7e8",
      "metadata": {
        "id": "dd06d7e8"
      },
      "source": [
        "끝!"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}